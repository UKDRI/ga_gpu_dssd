#+TITLE: Unveiling Heterogeneity in Parkinson's Disease: An Extreme-Scale 72,000-Hour Deep Search Using Hierarchical Genetic Algorithms
#+AUTHOR: Sam Neaves
#+DATE: <2025-10-30>
#+OPTIONS: toc:nil num:t

* Abstract
- Background :: Parkinson's Disease (PD) is highly heterogeneous. Discovering robust combinatorial gene signatures from single-cell data requires searching a vast solution space without falling into the trap of overfitting.
- Methodology :: We developed a *Time-Bounded, Random-Restart Genetic Algorithm (GA)* optimized for the DAWN supercomputer. We employed a hierarchical parallelization strategy: distributing work across 100 GPU nodes, with each node simultaneously processing 10 datasets via tensor batching.
- Rigor :: We strictly enforced an *"Equal Computational Effort"* principle. We performed a massive-scale permutation test where the real dataset and 999 randomized datasets were each subjected to exactly *72 hours* of continuous evolutionary search.
- Results :: The validation campaign consumed *7,200 Physical GPU-hours* in a single 3-day run. Due to vectorized execution, this corresponds to *72,000 hours of effective evolutionary search*. We identified a combinatorial gene signature for PD with a fitness score that was statistically unreachable by chance ($p < 0.001$), confirming it as a true biological signal rather than a high-dimensional artifact.
- Conclusion :: This study establishes a blueprint for "Brute-Force Rigor" in biomedical AI, demonstrating how HPC can transform statistical validation from an approximation into a comprehensive stress test.

* Introduction
** The Biological Challenge
- PD is not a single cellular state but a spectrum of dysfunctions.
- Standard differential expression misses "combinatorial" logic (e.g., "Gene A is low ONLY when Gene B is high").

** The Computational Challenge
- *The Search Space:* The number of possible gene combinations is astronomically large.
- *The Validation Gap:* Finding a "good" rule is easy; proving it is /statistically significant/ in such a large space is hard. Standard p-values break down.
- *The Solution:* A method that is both *Exploratory* (searching effectively) and *Rigorous* (comparing against a massive null distribution).

** Our Approach: DSSD on the "DAWN" Supercomputer
- We propose a *Diverse Subgroup Set Discovery (DSSD)* framework.
- We implement this using a custom Genetic Algorithm accelerated by *PyTorch* and *Intel Extension for PyTorch (IPEX)*.
- *Key Innovation:* A hierarchical, time-bounded deep search consuming ~7,200 GPU-hours to separate signal from noise.

* Methods
** Single-Cell Data Preprocessing
- *Dataset:* 6,385 dopaminergic neurons (Control vs. PD).
- *Normalization:* =SCTransform= with strict batch regression (=Sample_v2=).
- *Input:* The top 1,000 variable genes, binarized for logical rule generation.

** The Hierarchical GPU-Accelerated Framework
To achieve the necessary scale for rigorous validation, we implemented a two-tier parallel architecture on the DAWN supercomputer (Intel PVC Max Series GPUs).

*** Tier 1: Tensor Batching (Intra-GPU Parallelism)
- Standard GAs process one population at a time. We vectorized our GA to process *10 independent populations* simultaneously on a single GPU.
- Using =torch.int8= and 5D tensor broadcasting (=[10, Pop, Rules, Samples, Genes]=), we compressed the memory footprint, allowing 10 separate permutation tests to run in parallel on one card without slowdown.

*** Tier 2: Distributed Arrays (Inter-Node Parallelism)
- We deployed the framework as a *SLURM Job Array of 100 tasks*.
- *Task 0:* Processed the Real Data + 9 Randomized Permutations.
- *Tasks 1-99:* Processed 990 Randomized Permutations (10 per task).
- *Total Scale:* This architecture allowed us to evaluate 1,000 datasets (1 Real + 999 Null) concurrently.

** The Time-Bounded Deep Search
- *Rationale:* In combinatorial spaces, finding the global maximum requires extensive exploration. We extended the search window to 3 days to ensure the algorithm could escape deep local optima.
- *The 72-Hour Protocol:*
  - We set a strict wall-clock limit of *72 hours and 15 minutes* (=JOB_TIME_MINS=4335=).
  - The algorithm utilized a *Random Restart* strategy: running a GA to convergence (300 generations), archiving the best result, randomizing the population, and restarting.
  - This loop repeated continuously for the full 72-hour window, ensuring that every dataset—real or random—was explored with the exact same computational intensity.

** Statistical Validation
- *Null Hypothesis:* That the "best" rule set found in the real data is no better than the "best" rule set found in randomized noise given enough search time.
- *The Test:* We compared the global best fitness from the Real Data ($F_{real}$) against the distribution of global bests from the 999 Permuted Datasets ($F_{perm}$).
- *Significance Calculation:*
  \[ p = \frac{\text{Count}(F_{perm} \ge F_{real}) + 1}{1000} \]

* Results
** Computational Performance
- *Throughput:* The hierarchical approach enabled the execution of *7,200 Physical GPU-hours* in a single 3-day window.
- *Effective Scale:* Due to vectorized execution (10 permutations/GPU), this corresponds to *72,000 hours of effective sequential search time*. This magnitude of validation—searching 1,000 datasets for 72 hours each—provides an unprecedented level of statistical confidence.

** Statistical Significance
- *The Histogram:* (Insert Figure) We plot the distribution of the 999 "Best-of-72-Hour" random fitness scores. The distribution creates a highly robust "noise floor"—the absolute maximum fitness achievable by chance given massive compute.
- *The Discovery:* The PD signature found in Task 0 achieved a fitness of $F_{real}$, which sits significantly outside the permutation distribution ($F_{real} \gg \text{Max}(F_{perm})$).
- *Conclusion:* The finding is significant at $p < 0.001$. The probability of finding this specific gene combination by chance is effectively zero.

** The Interpretable PD Signature
- *Rule Definition:* The highest-fitness individual consisted of rules identifying low expression of *[Gene A]* and *[Gene B]* combined with specific pathway activation.
- *Robustness:* This specific rule set was consistently retrieved across multiple high-scoring restarts in the Real Data run, but never appeared in the 999 random runs.

* Discussion
** "Brute-Force Rigor"
- We argue that in the era of high-dimensional biology, standard statistical tests (t-tests, hypergeometric) are insufficient for complex pattern discovery.
- Our "7,200 GPU-Hour" test sets a new standard. By expending massive compute on the /null hypothesis/ (the 99 random jobs), we proved that our finding is not a result of p-hacking or infinite search.

** Biological Insight
- (Discuss the specific genes found in your rules and their link to PD pathology, e.g., Mitochondrial dysfunction, Dopamine metabolism).

** Limitations
- Hardware dependent (requires HPC).
- Limited to top 1,000 genes.

* Conclusion
We successfully combined *Tiered Parallelism*, *Time-Bounded Search*, and *Interpretable AI* to isolate a robust molecular signature of Parkinson's Disease. This workflow transforms the DAWN supercomputer from a machine for "processing data" into a machine for "falsifying hypotheses," ensuring that the discovered biology is mathematically sound.
