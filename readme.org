#+title: GPU-Accelerated Subgroup Discovery (Genetic Algorithm)
#+author: Sam Neaves
#+date: <2025-10-30>
#+options: toc:2 num:nil
#+property: header-args :eval never-export

* Project Goal

This project uses a parallel Genetic Algorithm (GA) to discover statistically significant, interpretable rule sets (subgroups) from high-dimensional single-cell RNA-seq data.  
The goal is to identify combinations of gene expression states (e.g., =Gene A = 1 AND Gene B = 0=) that are strongly associated with the Parkinson's disease (PD) phenotype.

The analysis runs on the *DAWN HPC*, utilizing its *Intel PVC GPUs* for accelerated computation.

* Statistical Method: "Best-of-N-Repeats" Permutation Test

To ensure our results are statistically significant and not random artifacts, we use a rigorous permutation test.

** The Problem
A single GA run can get stuck in a *local optimum* (a good-but-not-great result).  
If we run the GA 100 times on the real data but only once on the permuted data, it’s an unfair comparison.

** The Solution
We apply the *exact same computational effort* to the real data as to each of the 999 permuted datasets.

** Our Method
1. Create 1000 datasets (1 real, 999 permuted).  
2. For each dataset, run the GA =num_repeats= times (e.g., 200 times) from different random starting points.  
3. Record the *single best fitness score* from those 200 repeats.  
4. This yields one "best-of-200" score for the real data and 999 "best-of-200" scores for the permuted data.  
5. The *p-value* is computed by comparing the real score to the distribution of the permuted scores.

* File Descriptions

| File                  | Description                                                                                  |
|------------------------+----------------------------------------------------------------------------------------------|
| =submit_ga.sh=         | Main run script. Creates output dirs and submits jobs to SLURM.                             |
| =run_ga_array.slurm=   | Internal SLURM job array script (=--array=0-999=). Sets up modules for each job.            |
| =simple_ga_array.py=   | Core Python GA worker. One instance runs per task.                                          |
| =gather_results.py=    | Post-processing analysis script to compute p-values and summary statistics.                 |
| =sc_data/=             | Directory containing input data (=X_binary.csv=, =y_labels.csv=).                           |

** Task Breakdown
- *Task 0:* Loads the real data, runs GA =num_repeats= times, saves best result to =ga_results_0.txt=  
- *Tasks 1–999:* Each loads a permuted dataset, runs GA =num_repeats= times, and saves best result to =ga_results_X.txt=

* Key Parameters (Inside =simple_ga_array.py=)

#+begin_src python
# Main GA configuration
num_repeats = 200                # Number of GA runs per permutation (Best-of-N)
num_generations = 1000           # Generations per GA run
population_size = 55             # Max population fitting in GPU memory
gene_inclusion_probability = 0.01  # Controls sparsity (~10 genes per rule)
#+end_src

* How to Run the Full Experiment (10,000 GPU-hours)

** Step 1: Clean Up & Prepare
Ensure your scripts (=submit_ga.sh=, =run_ga_array.slurm=, =simple_ga_array.py=, =gather_results.py=)  
are in your main directory: =/home/dn-neav1/gpu_ga=

#+begin_src bash
# Clean up previous runs
rm -f slurm-*.out
rm -f ga_results_*.txt
rm -rf ga_run_*
#+end_src

** Step 2: Create a New Output Directory
Create a directory for this run. All logs and results will go here.

#+begin_src bash
mkdir ga_run_pop55_gen1000_rep200
#+end_src

** Step 3: Submit the Job Array
Submit the full GA array job via SLURM.  
The first argument is your output directory, the second is the walltime (safe limit: 10 hours 15 minutes).

#+begin_src bash
# Usage:
./submit_ga.sh <output_dir> --time=HH:MM:SS

# Example:
./submit_ga.sh ga_run_pop55_gen1000_rep200 --time=10:15:00
#+end_src

** Step 4: Monitor the Job

Check job status:
#+begin_src bash
squeue -u $(whoami)
#+end_src

You’ll see ~64 jobs running/configuring (R/CF) and ~936 pending (PD) — this is due to the 64-GPU user limit.  
Jobs will complete sequentially over ~6.5 days.

View SLURM output logs:
#+begin_src bash
ls ga_run_pop55_gen1000_rep200/logs/output/
#+end_src

* How to Analyze Results (After Job Array Completes)

Once =squeue -u $(whoami)= shows no jobs left, all 1000 tasks are done.

** Step 1: Run the Gather Script
Run the summarization step on the login node.

#+begin_src bash
# Usage:
python3 gather_results.py <output_dir>

# Example:
python3 gather_results.py ga_run_pop55_gen1000_rep200
#+end_src

** Step 2: Check the Output
Example output:

#+begin_example
--- Final Permutation Test (Best-of-N Repeats) ---
Real Data Best-of-N Fitness: 0.030902 (from Task 0)
Total Permuted Runs Found: 999
Max Permuted Fitness (Best-of-N): 0.024391
Avg Permuted Fitness (Best-of-N): 0.014825
Std Dev Permuted Fitness (Best-of-N): 0.003419

Empirical p-value: (0 + 1) / (999 + 1) = 0.0010

--- Top Rule Set (from Task 0) ---
  Rule 1: TPPP=0 AND SEC62=0 AND COX6C=0 AND ...
  Rule 2: NLGN4Y=0 AND COL5A1=0 AND FUNDC2=0 AND ...
#+end_example

** Step 3: View the Saved Summary
A file named =final_summary_best_of_N.txt= will be saved inside your output directory:

#+begin_src bash
cat ga_run_pop55_gen1000_rep200/final_summary_best_of_N.txt
#+end_src

---

*Notes:*  
This Org file can be exported to PDF or HTML with:
#+begin_src emacs-lisp
M-x org-export-dispatch
#+end_src
Choose `h` for HTML or `p` for PDF (via LaTeX).
