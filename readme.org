#+title: GPU-Accelerated DSSD and EMM for Single-Cell Analysis
#+author: Sam Neaves
#+date: <2025-11-16>
#+options: toc:2 num:nil
#+property: header-args :eval never-export

Project Goal

This project implements a multi-phase computational pipeline to identify and characterize disease-associated cellular states from high-dimensional single-cell RNA-seq data.

Phase 1 (Subgroup Discovery): Uses a custom-built, GPU-accelerated Genetic Algorithm (GA) to discover statistically significant, diverse subgroup sets (DSSD). These are interpretable rule sets (e.g., =Gene A=1 AND Gene B=0=) that are strongly associated with the Parkinson's disease (PD) phenotype.

Phase 2 (Exceptional Model Mining): Uses the DSSD program within an Apptainer container to identify regulatory networks. It searches for "description" genes that predict the expression of the biomarker set found in Phase 1, and contrasts these networks between PD and Control cells.

The analysis runs on the DAWN HPC, utilizing its Intel PVC GPUs for Phase 1 and its CPU nodes for Phase 2.

Phase 1: Genetic Algorithm (Subgroup Discovery)

** Statistical Method: Time-Limited Random-Restart Permutation Test

To find the best possible solution and ensure its statistical significance, we employ a "random-restart" (or "best-of-N-restarts") strategy. This addresses the "Winner's Curse" and the risk of the GA converging on a poor local optimum.

The Array Job: The simple_ga.sh script launches a SLURM job array of 100 tasks (Task 0 for real data, Tasks 1-99 for permuted data).

The Time Budget: Each of these 100 tasks is given a large time budget (e.g., 10 hours).

The "Random-Restart" Loop: Inside the 10-hour window, the simple_ga_array.py script runs a while(time.time()...) loop. This loop performs many (e.g., 150-200) independent, 300-generation GA "restarts."

Finding the "Champion": Each 10-hour job tracks the single best solution ("champion") it finds across all its restarts.

The Final p-value: The gather_results.py script compares the "champion of the real data" (from Task 0) to the "distribution of champions from the 99 permuted jobs" (from Tasks 1-99).

To further mitigate the "Winner's Curse," Task 0 tracks the Top 5 distinct, high-quality solutions it finds across all its restarts, presenting a more diverse set of final candidates.

** Key GA Parameters

=population_size = 55=: The maximum population size that can fit within the ~128GB of VRAM on a single PVC GPU, given our highly parallelized tensor operations.

=num_generations = 300=: The "depth" of a single GA run. This is optimized for fast local convergence.

=JOB_TIME_MINS= (in =run_ga_array.slurm=): The "breadth" of the search. A 10-hour (615 min) budget allows for hundreds of restarts, ensuring robust global exploration.

** GA File Descriptions

File

Description

=simple_ga.sh=

Main submission script. Launches the GA array and the dependent gather job.

=run_ga_array.slurm=

SLURM script for the main GA array job. Sets up the pytorch-gpu environment and passes the time limit.

=simple_ga_array.py=

Core Python GA worker. Runs the time-limited while loop of many restarts.

=ga_utils.py=

Shared Python functions for CPU-based rule decoding and stats calculation.

=run_gather.slurm=

SLURM script for the post-processing 'gather' job. Runs after the array finishes.

=gather_results.py=

Analyzes all ga_results_*.txt files. Computes p-value, Cover Redundancy, and generates plots.

=sc_data/=

Directory containing input data (=X_binary.csv=, =y_labels.csv=).

Phase 2: Exceptional Model Mining (EMM)

This phase uses the original DSSD program (running in wine inside an apptainer container) to perform EMM. It identifies "description" genes (potential regulators) that best predict the expression pattern of the biomarker set discovered in Phase 1. This analysis is run separately on the PD and Control cell populations to find "disease-specific network rewiring."

** EMM File Descriptions

File

Description

=wine-test-dssd.sif=

The apptainer container image containing the dssd.exe binaries.

=prepare_emm_data.py=

Python script to prepare data for DSSD. Splits X_binary.csv into PD/Ctrl groups and creates the required .arff and .emm files.

=dssd_pd.conf=

Config file for the DSSD run on PD patients. Sets measure = WKL.

=dssd_ctrl.conf=

Config file for the DSSD run on Control patients.

=run_emm.slurm=

SLURM script to launch the apptainer job on a pvc9 node. It runs the EMM analysis for both PD and Ctrl groups.

How to Run the Full Pipeline

** Step 1: Run the Phase 1 GA (Biomarker Discovery)

Set parameters in =run_ga_array.slurm= (e.g., --array=0-99, --time=10:15:00).

Make scripts executable:
#+begin_src bash
chmod +x simple_ga.sh
chmod +x run_ga_array.slurm
chmod +x run_gather.slurm
#+end_src

Launch the automated workflow. This single command will run the 100-job array and automatically run the gather script when it's done.
#+begin_src bash
./simple_ga.sh ./11nov_prod_10hr_100jobs
#+end_src

When complete, check the results in the output directory:
#+begin_src bash
ls ./11nov_prod_10hr_100jobs/

final_summary.txt  fitness_histogram.png  subgroup_analysis.png ...

#+end_src

** Step 2: Run the Phase 2 EMM (Regulatory Network Discovery)

First, prepare the data. Use the --k argument to select a random subset of description genes (e.g., 900) for the test.
#+begin_src bash

(Load the conda env first)

module purge
module load default-dawn
module load intelpython-conda
conda activate pytorch-gpu

Run the prep script

python3 prepare_emm_data.py --k 900
#+end_src

Make the EMM run script executable:
#+begin_src bash
chmod +x run_emm.slurm
#+end_src

Submit the EMM job to SLURM.
#+begin_src bash
sbatch run_emm.slurm
#+end_src

Check the results. DSSD will create its own output folders inside emm_results/.
#+begin_src bash
ls ./emm_results/

pd_run-1762972624/  ctrl_run-1762972625/ ...

#+end_src
